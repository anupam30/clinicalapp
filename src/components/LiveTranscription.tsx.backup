import { useEffect, useRef, useState } from 'react';
import { Mic, MicOff, AlertCircle, Cloud, Zap, Radio } from 'lucide-react';
import { Button } from './ui/button';
import { Alert, AlertDescription } from './ui/alert';
import { Dialog, DialogContent, DialogHeader, DialogTitle, DialogDescription, DialogFooter } from './ui/dialog';
import { Checkbox } from './ui/checkbox';
import { Select, SelectContent, SelectItem, SelectTrigger, SelectValue } from './ui/select';
import { useDeepgram } from '@/hooks/useDeepgram';
import { assemblyaiProvider } from '@/services/stt/AssemblyAIProvider';
import { VoiceAnalyzer } from '@/services/speaker-detection/VoiceAnalyzer';

type STTProvider = 'deepgram' | 'browser' | 'assemblyai';

export interface Message {
  speaker: 'Doctor' | 'Patient';
  text: string;
  timestamp?: number;
}

interface LiveTranscriptionProps {
  onTranscriptUpdate: (transcript: string) => void;
  onMessagesUpdate?: (messages: Message[]) => void;
  isRecording: boolean;
  onRecordingChange: (recording: boolean) => void;
  selectedLanguage: string;
  onLanguageChange: (language: string) => void;
}

export function LiveTranscription({ 
  onTranscriptUpdate,
  onMessagesUpdate,
  isRecording, 
  onRecordingChange,
  selectedLanguage,
  onLanguageChange
}: LiveTranscriptionProps) {
  const [transcript, setTranscript] = useState('');
  const [messages, setMessages] = useState<Message[]>([]);
  const [interimTranscript, setInterimTranscript] = useState('');
  const [error, setError] = useState<string | null>(null);
  const [isSupported, setIsSupported] = useState(true);
  const [showConsentDialog, setShowConsentDialog] = useState(false);
  const [consentGiven, setConsentGiven] = useState(false);
  const [currentProvider, setCurrentProvider] = useState<STTProvider>('deepgram');
  const [isAssemblyAIInitialized, setIsAssemblyAIInitialized] = useState(false);
  const [isAssemblyAILoading, setIsAssemblyAILoading] = useState(false);
  
  const recognitionRef = useRef<any>(null);
  const micStreamRef = useRef<MediaStream | null>(null);
  const audioAnalysisRef = useRef<any>(null);
  const transcriptRef = useRef('');
  const messagesRef = useRef<Message[]>([]);
  const voiceAnalyzerRef = useRef<VoiceAnalyzer | null>(null);
  const currentSpeakerRef = useRef<'Patient' | 'Doctor'>('Patient');
  const lastSpeakerChangeTimeRef = useRef(0);
  const minSpeakerChangeDurationRef = useRef(2000); // Minimum 2 seconds before switching speaker

  // Deepgram hook initialization
  const deepgram = useDeepgram({
    onTranscriptUpdate: (text: string, isFinal: boolean) => {
      if (currentProvider !== 'deepgram') return;
      
      if (isFinal && text.trim()) {
        // Detect speaker based on content
        const speaker = detectSpeakerFromContent(text);
        
        // Update current speaker if changed
        if (speaker !== currentSpeakerRef.current && 
            Date.now() - lastSpeakerChangeTimeRef.current > minSpeakerChangeDurationRef.current) {
          currentSpeakerRef.current = speaker;
          lastSpeakerChangeTimeRef.current = Date.now();
          console.log(`üîä Speaker switched to: ${speaker}`);
        }
        
        // Add message to array
        const newMessage: Message = {
          speaker: currentSpeakerRef.current,
          text: text.trim(),
          timestamp: Date.now()
        };
        
        messagesRef.current.push(newMessage);
        setMessages([...messagesRef.current]);
        onMessagesUpdate?.([...messagesRef.current]);
        
        // Update transcript display
        const formattedText = `${currentSpeakerRef.current}: ${text.trim()}\n`;
        transcriptRef.current += formattedText;
        setTranscript(transcriptRef.current);
        onTranscriptUpdate(transcriptRef.current);
      } else {
        setInterimTranscript(text);
      }
    },
    onError: (deepgramError) => {
      console.error('Deepgram error:', deepgramError);
      // Fallback to browser STT
      setCurrentProvider('browser');
      setError(`Deepgram unavailable, switched to browser STT: ${deepgramError}`);
      // Start browser recognition
      if (recognitionRef.current && isRecording) {
        setTimeout(() => recognitionRef.current?.start(), 100);
      }
    },
  });

  // Enhanced speaker detection - uses both acoustic analysis and content keywords
  const detectSpeakerFromContent = (text: string): 'Doctor' | 'Patient' => {
    const lowerText = text.toLowerCase();
    
    // Doctor indicators (questions, commands, medical terms)
    const doctorPatterns = [
      /^(what|when|where|which|how|kya|kab|kaise)\s/i,  // Questions
      /^(tell|show|explain|describe|dikhao|batao|chaliye)\s/i,  // Commands
      /(medicine|tablet|prescription|diagnosis|test|investigation|examination|injection|surgery)\b/i,
      /(checkup|follow.?up|come back|aana|dobara|phir.?se)\b/i,
      /^(okay|alright|fine|theek|bilkul|haan)\s/i,  // Acknowledgments
      /(dosage|frequency|duration|medication|dose)\b/i,  // Medical dosing terms
    ];
    
    // Patient indicators (complaints, symptoms, personal references, short responses)
    const patientPatterns = [
      /^(i have|i am|my|mujhe|mere|mera|meri)\s/i,  // First person
      /(pain|dard|fever|bukhar|cough|khasi|weakness|kamzori|symptom|ache|swelling)\b/i,
      /(since|se.*hai|days|weeks|mahine|saal|last|ago)\b/i,  // Duration
      /^(yes|no|haan|nahi|ji|ok|okay|alright|bilkul|thik)$/i,  // Short responses
      /(doctor|sir|madam|ji|aap)\b/i,  // Addressing doctor
    ];
    
    let doctorScore = 0;
    let patientScore = 0;
    
    // Check patterns
    doctorPatterns.forEach(pattern => {
      if (pattern.test(lowerText)) doctorScore += 2;
    });
    
    patientPatterns.forEach(pattern => {
      if (pattern.test(lowerText)) patientScore += 2;
    });
    
    // Question marks usually indicate doctor/inquisitive
    if (text.includes('?')) doctorScore += 3;
    
    // If text is very short and is a yes/no response, likely patient
    if (text.length < 10 && /^(yes|no|haan|nahi|ji|ok|okay|alright|bilkul|thik|theek|bahut)$/i.test(text.trim())) {
      patientScore += 3;
    }
    
    // If previous speaker was doctor and this is short, likely patient response
    if (currentSpeakerRef.current === 'Doctor' && text.length < 20 && !text.includes('?')) {
      patientScore += 2;
    }
    
    const detected = doctorScore > patientScore ? 'Doctor' : 'Patient';
    console.log(`üé§ Speaker: "${text.substring(0, 40)}..." -> Doctor:${doctorScore} Patient:${patientScore} = ${detected}`);
    
    return detected;
  };

  const detectSpeaker = (): 'Doctor' | 'Patient' => {
    return currentSpeakerRef.current;
  };

  useEffect(() => {
    // Initialize AssemblyAI provider
    const initAssemblyAI = async () => {
      try {
        setIsAssemblyAILoading(true);
        await assemblyaiProvider.initialize();
        setIsAssemblyAIInitialized(true);
        console.log('‚úì AssemblyAI provider initialized');
      } catch (err) {
        console.warn('AssemblyAI initialization failed:', err);
        setIsAssemblyAIInitialized(false);
      } finally {
        setIsAssemblyAILoading(false);
      }
    };

    initAssemblyAI();
  }, []);

  useEffect(() => {
    // Check if browser supports Web Speech API
    const SpeechRecognition = (window as any).SpeechRecognition || (window as any).webkitSpeechRecognition;
    
    if (!SpeechRecognition) {
      setIsSupported(false);
      setError('Speech recognition is not supported in this browser. Please use Chrome or Edge.');
      return;
    }

    const recognition = new SpeechRecognition();
    recognition.continuous = true;
    recognition.interimResults = true;
    
    // Set language based on selection
    const langMap: { [key: string]: string } = {
      'hindi': 'hi-IN',
      'english': 'en-IN',
      'hinglish': 'en-IN' // Use Indian English locale which tends to handle Hinglish better
    };
    recognition.lang = langMap[selectedLanguage] || 'hi-IN';

    recognition.onresult = (event: any) => {
      let interim = '';
      let final = '';

      for (let i = event.resultIndex; i < event.results.length; i++) {
        const transcriptText = event.results[i][0].transcript;
        if (event.results[i].isFinal) {
          final += transcriptText + ' ';
        } else {
          interim += transcriptText;
        }
      }

      if (final && final.trim()) {
        // Detect speaker based on content
        const speaker = detectSpeakerFromContent(final);
        
        // Update current speaker if changed
        if (speaker !== currentSpeakerRef.current && 
            Date.now() - lastSpeakerChangeTimeRef.current > minSpeakerChangeDurationRef.current) {
          currentSpeakerRef.current = speaker;
          lastSpeakerChangeTimeRef.current = Date.now();
        }
        
        // Add message to array
        const newMessage: Message = {
          speaker: currentSpeakerRef.current,
          text: final.trim(),
          timestamp: Date.now()
        };
        
        messagesRef.current.push(newMessage);
        setMessages([...messagesRef.current]);
        onMessagesUpdate?.([...messagesRef.current]);
        
        // Update transcript display
        const formattedText = `${currentSpeakerRef.current}: ${final.trim()}\n`;
        transcriptRef.current += formattedText;
        setTranscript(transcriptRef.current);
        onTranscriptUpdate(transcriptRef.current);
      }
      
      setInterimTranscript(interim);
    };

    recognition.onerror = (event: any) => {
      console.error('Speech recognition error:', event.error);
      
      if (event.error === 'no-speech') {
        // Restart if no speech detected
        if (isRecording) {
          recognition.stop();
          setTimeout(() => recognition.start(), 100);
        }
      } else if (event.error === 'not-allowed') {
        setError('Microphone access denied. Please allow microphone access.');
        onRecordingChange(false);
      } else {
        setError(`Error: ${event.error}`);
      }
    };

    recognition.onend = () => {
      // Restart if still recording
      if (isRecording) {
        try {
          recognition.start();
        } catch (err) {
          console.error('Error restarting recognition:', err);
        }
      }
    };

    recognitionRef.current = recognition;

    return () => {
      if (recognitionRef.current) {
        recognitionRef.current.stop();
      }
    };
  }, [selectedLanguage]);

  const handleConsentAndStart = async () => {
    if (!consentGiven) {
      return;
    }

    setShowConsentDialog(false);
    setError(null);

    try {
      // Request microphone access with explicit constraints
      console.log('Requesting microphone access...');
      console.log('Browser:', navigator.userAgent.substring(0, 100));
      
      const audioConstraints = {
        audio: {
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true,
          sampleRate: { ideal: 16000 },
          channelCount: { ideal: 1 },
        }
      };
      
      console.log('Audio constraints:', JSON.stringify(audioConstraints));
      
      try {
        const stream = await navigator.mediaDevices.getUserMedia(audioConstraints);
        micStreamRef.current = stream;
        console.log('‚úÖ Microphone access granted!');
        console.log('Microphone details:', {
          tracks: stream.getTracks().length,
          audioTracks: stream.getAudioTracks().length,
          enabled: stream.getAudioTracks()[0]?.enabled,
          settings: stream.getAudioTracks()[0]?.getSettings?.()
        });
      } catch (strictError) {
        console.warn('Strict audio constraints failed, trying with minimal constraints:', strictError);
        // Fallback to minimal constraints
        const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
        micStreamRef.current = stream;
        console.log('‚úÖ Microphone access granted (with minimal constraints)');
      }
      
      // Reset transcript and messages for new recording
      transcriptRef.current = '';
      messagesRef.current = [];
      currentSpeakerRef.current = 'Patient';
      lastSpeakerChangeTimeRef.current = 0;
      setTranscript('');
      setMessages([]);

      // Use user's selected provider
      if (currentProvider === 'deepgram' && deepgram.isInitialized) {
        console.log('Starting Deepgram...');
        try {
          await deepgram.startListening(micStreamRef.current!);
          console.log('‚úì Using Deepgram (Cloud STT)');
        } catch (deepgramError) {
          console.error('Deepgram start error:', deepgramError);
          // Fallback to browser
          setCurrentProvider('browser');
          setError('Deepgram unavailable, using browser STT as fallback');
          if (recognitionRef.current) {
            recognitionRef.current.start();
          }
        }
      } else if (currentProvider === 'deepgram' && !deepgram.isInitialized) {
        // User selected Deepgram but it's not available
        console.log('Deepgram not initialized, falling back to browser STT');
        setCurrentProvider('browser');
        setError('Deepgram not available, using browser STT as fallback');
        if (recognitionRef.current) {
          recognitionRef.current.start();
          // start audio analysis in parallel to recognition (for pitch/VAD)
          startAudioAnalysis();
        }
      } else if (currentProvider === 'assemblyai' && isAssemblyAIInitialized) {
        console.log('Starting AssemblyAI...');
        try {
          await assemblyaiProvider.startListening(
            (text: string, isFinal: boolean) => {
              if (isFinal && text.trim()) {
                // Detect speaker based on content
                const speaker = detectSpeakerFromContent(text);
                
                // Update current speaker if changed
                if (speaker !== currentSpeakerRef.current && 
                    Date.now() - lastSpeakerChangeTimeRef.current > minSpeakerChangeDurationRef.current) {
                  currentSpeakerRef.current = speaker;
                  lastSpeakerChangeTimeRef.current = Date.now();
                }
                
                // Add message to array
                const newMessage: Message = {
                  speaker: currentSpeakerRef.current,
                  text: text.trim(),
                  timestamp: Date.now()
                };
                
                messagesRef.current.push(newMessage);
                setMessages([...messagesRef.current]);
                onMessagesUpdate?.([...messagesRef.current]);
                
                // Update transcript display
                const formattedText = `${currentSpeakerRef.current}: ${text.trim()}\n`;
                transcriptRef.current += formattedText;
                setTranscript(transcriptRef.current);
                onTranscriptUpdate(transcriptRef.current);
              } else {
                setInterimTranscript(text);
              }
            },
            (error: string) => {
              console.error('AssemblyAI error:', error);
              // Fallback to browser
              setCurrentProvider('browser');
              setError(`AssemblyAI unavailable, switched to browser STT: ${error}`);
              if (recognitionRef.current && isRecording) {
                setTimeout(() => recognitionRef.current?.start(), 100);
              }
            },
            micStreamRef.current! // Pass the existing stream
          );
          console.log('‚úì Using AssemblyAI (Cloud STT)');
        } catch (assemblyError) {
          console.error('AssemblyAI start error:', assemblyError);
          // Fallback to browser
          setCurrentProvider('browser');
          setError('AssemblyAI unavailable, using browser STT as fallback');
          if (recognitionRef.current) {
            recognitionRef.current.start();
          }
        }
      } else if (currentProvider === 'assemblyai' && !isAssemblyAIInitialized) {
        // User selected AssemblyAI but it's not available
        console.log('AssemblyAI not initialized, falling back to browser STT');
        setCurrentProvider('browser');
        setError('AssemblyAI not available, using browser STT as fallback');
        if (recognitionRef.current) {
          recognitionRef.current.start();
          startAudioAnalysis();
        }
      } else {
        // User selected browser STT
        console.log('Using browser STT');
        if (recognitionRef.current) {
          recognitionRef.current.start();
          startAudioAnalysis();
        }
      }
      
      onRecordingChange(true);
    } catch (err: any) {
      console.error('‚ùå Microphone access error:', err);
      console.error('Error details:', {
        name: err?.name,
        message: err?.message,
        code: err?.code,
        constraint: err?.constraint,
        type: typeof err,
      });
      
      // Provide detailed error message based on error type
      let errorMsg = 'Microphone access denied.';
      
      if (err?.name === 'NotAllowedError') {
        errorMsg = '‚ùå Permission Denied: Microphone access blocked. Check browser settings or reload and grant permission when prompted.';
      } else if (err?.name === 'NotFoundError' || err?.name === 'DevicesNotFoundError') {
        errorMsg = '‚ùå No Microphone Found: No microphone device detected. Check hardware connections.';
      } else if (err?.name === 'NotReadableError' || err?.name === 'AbortError') {
        errorMsg = '‚ùå Microphone In Use: Microphone is being used by another application. Close other audio apps and try again.';
      } else if (err?.name === 'OverconstrainedError') {
        errorMsg = '‚ùå Microphone Constraints Error: Your microphone doesn\'t support the requested settings. Reloading...';
      } else if (err?.name === 'TypeError') {
        errorMsg = '‚ùå Browser Error: getUserMedia not supported or HTTPS required. Use HTTPS or a supported browser.';
      } else if (err?.message?.includes('Permission denied')) {
        errorMsg = '‚ùå System Permission Denied: Operating system microphone access denied. Check OS settings.';
      } else {
        errorMsg = `‚ùå Error: ${err?.message || 'Unknown microphone error'}`;
      }
      
      setError(errorMsg);
    }
  };

  // Acoustic-based real speaker detection using VoiceAnalyzer
  const startAudioAnalysis = () => {
    try {
      if (!micStreamRef.current) return;
      
      console.log('[Audio Analysis] Starting acoustic speaker detection...');
      
      const AudioCtx = (window as any).AudioContext || (window as any).webkitAudioContext;
      const actx = new AudioCtx();
      const src = actx.createMediaStreamSource(micStreamRef.current);
      
      // Create analyser for voice characteristics
      const analyser = actx.createAnalyser();
      analyser.fftSize = 2048;
      analyser.smoothingTimeConstant = 0.3;
      
      // Initialize voice analyzer for speaker detection
      voiceAnalyzerRef.current = new VoiceAnalyzer(actx.sampleRate);
      currentSpeakerRef.current = 'Patient'; // Start with patient
      
      // Create processor for real-time analysis
      const processor = actx.createScriptProcessor(2048, 1, 1);
      
      processor.onaudioprocess = (ev: any) => {
        const data = ev.inputBuffer.getChannelData(0);
        
        // Skip silent frames
        let sum = 0;
        for (let i = 0; i < data.length; i++) sum += data[i] * data[i];
        const rms = Math.sqrt(sum / data.length);
        
        if (rms < 0.005) return; // Skip if too quiet
        
        // Analyze acoustic characteristics
        const analyzer = voiceAnalyzerRef.current;
        if (analyzer) {
          const { speaker, changeEvent } = analyzer.analyzeFrame(data);
          
          // Check if speaker changed
          if (changeEvent && Date.now() - lastSpeakerChangeTimeRef.current > minSpeakerChangeDurationRef.current) {
            const newSpeaker = changeEvent.newSpeaker === 0 ? 'Patient' : 'Doctor';
            currentSpeakerRef.current = newSpeaker;
            lastSpeakerChangeTimeRef.current = Date.now();
            
            console.log(`üîä Speaker change detected: ${newSpeaker}`, {
              confidence: changeEvent.confidence.toFixed(2),
              pitch: changeEvent.features.pitch.toFixed(0),
              energy: changeEvent.features.energy.toFixed(4),
              spectral: changeEvent.features.spectralCentroid.toFixed(0),
            });
          }
        }
      };
      
      src.connect(analyser);
      analyser.connect(processor);
      processor.connect(actx.destination);
      
      audioAnalysisRef.current = { actx, processor, analyser };
      console.log('[Audio Analysis] Acoustic analysis initialized');
    } catch (err) {
      console.error('Failed to start audio analysis:', err);
    }
  };

  const stopAudioAnalysis = () => {
    try {
      const o = audioAnalysisRef.current;
      if (o) {
        try { o.processor.disconnect(); } catch (e) {}
        try { o.analyser.disconnect(); } catch (e) {}
        try { o.actx.close(); } catch (e) {}
        audioAnalysisRef.current = null;
      }
      if (voiceAnalyzerRef.current) {
        voiceAnalyzerRef.current.reset();
        voiceAnalyzerRef.current = null;
      }
    } catch (err) {}
  };

  const toggleRecording = () => {
    if (!isRecording) {
      // Show consent dialog before starting
      setShowConsentDialog(true);
    } else {
      // Stop recording
      if (currentProvider === 'deepgram') {
        deepgram.stopListening();
      } else if (currentProvider === 'assemblyai') {
        assemblyaiProvider.stopListening();
      } else if (recognitionRef.current) {
        recognitionRef.current.stop();
      }
      // stop audio analysis if running
      try { stopAudioAnalysis(); } catch (e) {}
      setInterimTranscript('');
      onRecordingChange(false);
    }
  };

  if (!isSupported) {
    return (
      <div className="p-4">
        <Alert variant="destructive">
          <AlertCircle className="h-4 w-4" />
          <AlertDescription>
            Speech recognition is not supported in this browser. Please use Google Chrome or Microsoft Edge.
          </AlertDescription>
        </Alert>
      </div>
    );
  }

  return (
    <div className="h-full flex flex-col">
      {/* Header: Title + Record Button */}
      <div className="flex items-center gap-3 mb-4">
        <h3 className="text-sm font-semibold text-gray-700">Live Transcription</h3>
        
        {/* Start/Stop Recording Button - MAIN CONTROL */}
        <Button
          onClick={toggleRecording}
          className={`gap-2 ${isRecording ? 'bg-red-600 hover:bg-red-700' : 'bg-primary hover:bg-primary/90'}`}
        >
          {isRecording ? (
            <>
              <MicOff className="h-4 w-4" />
              Stop
            </>
          ) : (
            <>
              <Mic className="h-4 w-4" />
              Start Recording
            </>
          )}
        </Button>

        {/* Recording Badge (while recording) */}
        {isRecording && (
          <div className="flex items-center gap-1.5 px-3 py-1.5 bg-gray-100 rounded-full text-xs">
            {currentProvider === 'deepgram' ? (
              <>
                <Cloud className="h-3.5 w-3.5 text-blue-600" />
                <span className="font-medium text-blue-600">Deepgram (Cloud)</span>
              </>
            ) : currentProvider === 'assemblyai' ? (
              <>
                <Radio className="h-3.5 w-3.5 text-purple-600" />
                <span className="font-medium text-purple-600">AssemblyAI (Cloud)</span>
              </>
            ) : (
              <>
                <Zap className="h-3.5 w-3.5 text-amber-600" />
                <span className="font-medium text-amber-600">Browser STT</span>
              </>
            )}
          </div>
        )}
      </div>

      {/* Controls: Provider + Language Selection */}
      <div className="flex items-center gap-4 mb-4 pb-3 border-b">
        {/* STT Provider Selector */}
        {!isRecording && (
          <div className="flex items-center gap-2">
            <label className="text-xs text-gray-600 font-medium">Provider:</label>
            <Select 
              value={currentProvider} 
              onValueChange={(value: string) => setCurrentProvider(value as STTProvider)}
              disabled={isRecording}
            >
              <SelectTrigger className="w-[180px] h-8 bg-white">
                <SelectValue />
              </SelectTrigger>
              <SelectContent>
                <SelectItem value="deepgram">
                  {deepgram.isInitialized ? 'üåê Deepgram' : '‚ö†Ô∏è Deepgram'}
                </SelectItem>
                <SelectItem value="assemblyai">
                  {isAssemblyAIInitialized ? 'üìª AssemblyAI' : '‚ö†Ô∏è AssemblyAI'}
                </SelectItem>
                {/* AssemblyAI coming soon */}
                <SelectItem value="browser">‚ö° Browser STT</SelectItem>
              </SelectContent>
            </Select>
            {/* Status Indicator */}
            <div className="flex items-center gap-1 text-xs">
              {currentProvider === 'deepgram' ? (
                deepgram.isLoading ? (
                  <span className="text-gray-500">Initializing...</span>
                ) : deepgram.isInitialized ? (
                  <span className="text-green-600 flex items-center gap-1">
                    <span className="inline-block w-2 h-2 bg-green-600 rounded-full"></span>
                    Ready
                  </span>
                ) : (
                  <span className="text-amber-600 flex items-center gap-1">
                    <span className="inline-block w-2 h-2 bg-amber-600 rounded-full"></span>
                    Fallback
                  </span>
                )
              ) : currentProvider === 'assemblyai' ? (
                isAssemblyAILoading ? (
                  <span className="text-gray-500">Initializing...</span>
                ) : isAssemblyAIInitialized ? (
                  <span className="text-green-600 flex items-center gap-1">
                    <span className="inline-block w-2 h-2 bg-green-600 rounded-full"></span>
                    Ready
                  </span>
                ) : (
                  <span className="text-amber-600 flex items-center gap-1">
                    <span className="inline-block w-2 h-2 bg-amber-600 rounded-full"></span>
                    Fallback
                  </span>
                )
              ) : (
                <span className="text-green-600 flex items-center gap-1">
                  <span className="inline-block w-2 h-2 bg-green-600 rounded-full"></span>
                  Ready
                </span>
              )}
            </div>
          </div>
        )}

        {/* Language Selector */}
        <Select value={selectedLanguage} onValueChange={onLanguageChange} disabled={isRecording}>
          <SelectTrigger className="w-[140px] h-8 bg-white">
            <SelectValue />
          </SelectTrigger>
          <SelectContent>
            <SelectItem value="hinglish">Hinglish</SelectItem>
            <SelectItem value="hindi">‡§π‡§ø‡§Ç‡§¶‡•Ä (Hindi)</SelectItem>
            <SelectItem value="english">English</SelectItem>
          </SelectContent>
        </Select>
      </div>

      {error && (
        <Alert variant="destructive" className="mb-4">
          <AlertCircle className="h-4 w-4" />
          <AlertDescription>{error}</AlertDescription>
        </Alert>
      )}

      <div className="flex-1 bg-white border rounded-lg p-4 overflow-auto">
        {transcript ? (
          <div className="whitespace-pre-wrap font-mono text-sm">
            {transcript.split('\n').map((line, index) => {
              if (!line.trim()) return null;
              const isDoctor = line.startsWith('Doctor:');
              return (
                <div key={index} className={`mb-2 ${isDoctor ? 'text-blue-700 font-semibold' : 'text-gray-800'}`}>
                  {line}
                </div>
              );
            })}
            {interimTranscript && (
              <span className="text-gray-400 italic">{interimTranscript}</span>
            )}
          </div>
        ) : (
          <div className="h-full flex items-center justify-center text-gray-400">
            <div className="text-center">
              <Mic className="h-12 w-12 mx-auto mb-2 opacity-20" />
              <p className="text-sm">
                {isRecording 
                  ? 'Listening... Start speaking'
                  : 'Select language and click "Start Recording" to begin'}
              </p>
            </div>
          </div>
        )}
      </div>

      {/* Consent Dialog */}
      <Dialog open={showConsentDialog} onOpenChange={setShowConsentDialog}>
        <DialogContent>
          <DialogHeader>
            <DialogTitle>Patient Consent Required</DialogTitle>
            <DialogDescription>
              Before starting the recording, please confirm the following:
            </DialogDescription>
          </DialogHeader>
          <div className="space-y-4 my-4">
            <Alert>
              <AlertCircle className="h-4 w-4" />
              <AlertDescription>
                Recording patient conversations requires explicit consent and microphone access.
              </AlertDescription>
            </Alert>
            
            <div className="flex items-start space-x-3 p-4 bg-blue-50 border border-blue-200 rounded-md">
              <Checkbox 
                id="consent" 
                checked={consentGiven}
                onCheckedChange={(checked: boolean | 'indeterminate') => setConsentGiven(checked === true)}
              />
              <label htmlFor="consent" className="text-sm leading-relaxed cursor-pointer">
                I confirm that the patient has given explicit consent for this conversation to be recorded, 
                transcribed, and processed for medical documentation purposes. The patient understands 
                that this data will be stored securely and used only for their healthcare.
              </label>
            </div>
          </div>
          <DialogFooter>
            <Button variant="outline" onClick={() => setShowConsentDialog(false)}>
              Cancel
            </Button>
            <Button 
              onClick={handleConsentAndStart}
              disabled={!consentGiven}
              className="bg-primary hover:bg-primary/90"
            >
              Allow Microphone & Start Recording
            </Button>
          </DialogFooter>
        </DialogContent>
      </Dialog>
    </div>
  );
}